{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03 Digit Image Classification MNIST.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "v5J5g7zpCLdF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Ensure that all libraries are installed "
      ]
    },
    {
      "metadata": {
        "id": "TTn-dOAKCLdH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras import layers\n",
        "from keras import models\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "scuh8OJnCLdN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load MNIST dataset from the web"
      ]
    },
    {
      "metadata": {
        "id": "mixrJeYxCLdN",
        "colab_type": "code",
        "colab": {},
        "outputId": "d5179d42-f9fc-49a0-827e-01d29334aed0"
      },
      "cell_type": "code",
      "source": [
        "( train_images, train_labels ), ( test_images, test_labels ) = mnist.load_data()\n",
        "\n",
        "#outputting the shape of the data\n",
        "print('')\n",
        "print( 'train images shape: ' )\n",
        "print( train_images.shape )\n",
        "len( train_labels )\n",
        "print( train_labels )\n",
        "print('')\n",
        "\n",
        "print( 'test images shape: ' )\n",
        "print( test_images.shape )\n",
        "len( test_labels )\n",
        "print( test_labels )\n",
        "print('')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train images shape: \n",
            "(60000, 28, 28)\n",
            "[5 0 4 ... 5 6 8]\n",
            "\n",
            "test images shape: \n",
            "(10000, 28, 28)\n",
            "[7 2 1 ... 4 5 6]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n5Q0TUrGCLdS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualize a random image in the train dataset"
      ]
    },
    {
      "metadata": {
        "id": "c7B300HnCLdS",
        "colab_type": "code",
        "colab": {},
        "outputId": "0c30f54e-d2b9-4861-d17d-81f47cdbfe47"
      },
      "cell_type": "code",
      "source": [
        "plt.matshow( train_images[0], cmap = 'gray' )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "6nIxGD7cCLdX",
        "colab_type": "code",
        "colab": {},
        "outputId": "c3fa455e-0df5-4548-869c-84ca4304600e"
      },
      "cell_type": "code",
      "source": [
        "print( train_labels[0] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MtSwoHaQCLdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We start with reshaping the data for the convnet.\n",
        "\n",
        "notice we changed up the hyperparameters for this.\n",
        "Rather than using reshape: ( 60000, 28 * 28 ).\n",
        "We instead used: ( 60000, 28, 28, 1 ).\n",
        "\n",
        "This is because we need this format to fit the input shape of the model.\n",
        "\n",
        "Other than that we are still operating on the premise:\n",
        "\n",
        "1. shaping the data for the model.\n",
        "2. encoding the labels for both training and testing sets.\n",
        "3. compiling and training the model with training data.\n",
        "4. evaluating the model with the test data."
      ]
    },
    {
      "metadata": {
        "id": "13ZyN_wxCLdc",
        "colab_type": "code",
        "colab": {},
        "outputId": "6eefa8c4-b29c-4f9a-a44a-5b728a557917"
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape( ( 60000, 28, 28, 1 ) )\n",
        "train_images = train_images.astype( 'float32' ) / 255\n",
        "test_images = test_images.reshape( ( 10000, 28, 28, 1 ) )\n",
        "test_images = test_images.astype( 'float32' ) / 255\n",
        "train_labels = to_categorical( train_labels )\n",
        "test_labels = to_categorical( test_labels )\n",
        "print( train_labels.shape )\n",
        "print( train_labels[0] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 10)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OHUZowOTCLdg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the convolutional network. While you could use just the Dense layers to achieve digit identification. You could gain even more accuracy if you use convolutional network. So the idea is to use a filter that will create localized values which will piece together to identify digits.\n",
        "\n",
        "So here we have a couple of functions from layers:\n",
        "\n",
        "Conv2D is the convolutional layer.\n",
        "Conv2D( filters, ( filter_height, filter_width ), ( activation ), ( input_shape ) )\n",
        "\n",
        "The filter will move through the image pixel by pixel ( or \"convolves\") around the image picking up values.\n",
        "This will create an output of: 26 x 26.\n",
        "\n",
        "MaxPooling2D( height, width )\n",
        "MaxPooling layer just takes the highest values from the Convolutional layer and puts it in height x width. \n",
        "It will create an output of 2 x 2 format with the highest values from within the 4 squares.\n",
        "\n",
        "The last layer: model.add( layers.Dense( 10. activation = 'softmax' )\n",
        "\n",
        "So here we have 10 neurons which will associate, imagine digits between 0 and 9. Each neuron will contain values be between 0 - 1 and all of the values will sum to 1.\n",
        "\n",
        "So basically we have a probability for a digit prediction. The highest probability will be the model's prediction for the digit."
      ]
    },
    {
      "metadata": {
        "id": "ixyuwzFYCLdh",
        "colab_type": "code",
        "colab": {},
        "outputId": "a320fdcf-837b-4d49-b054-512290e3da16"
      },
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add( layers.Conv2D( 16, ( 3, 3 ), activation = 'relu', input_shape=( 28, 28, 1 ) ) )\n",
        "model.add( layers.MaxPooling2D( ( 2, 2 ) ) )\n",
        "model.add( layers.Conv2D( 4, ( 3, 3 ), activation = 'relu' ) )\n",
        "model.add( layers.Flatten() )\n",
        "model.add( layers.Dense( 10, activation = 'softmax' ) )\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 11, 11, 4)         580       \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 484)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                4850      \n",
            "=================================================================\n",
            "Total params: 5,590\n",
            "Trainable params: 5,590\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mj9I8_OzCLdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "compiling and training the model\n",
        "\n",
        "The model will be trained with the fit function."
      ]
    },
    {
      "metadata": {
        "id": "F3sh0E3OCLdn",
        "colab_type": "code",
        "colab": {},
        "outputId": "e3503a10-85ec-4c75-8f0b-19784ba1b5ca"
      },
      "cell_type": "code",
      "source": [
        "model.compile( optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = [ 'accuracy' ] )\n",
        "model.fit( train_images, train_labels, epochs = 4, batch_size = 64 )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "60000/60000 [==============================] - 36s 596us/step - loss: 0.3962 - acc: 0.88261s - loss: 0.4047 - acc - ETA: 1s - loss: 0.401\n",
            "Epoch 2/4\n",
            "60000/60000 [==============================] - 34s 566us/step - loss: 0.1484 - acc: 0.9571\n",
            "Epoch 3/4\n",
            "60000/60000 [==============================] - 35s 584us/step - loss: 0.1000 - acc: 0.9701\n",
            "Epoch 4/4\n",
            "60000/60000 [==============================] - 40s 674us/step - loss: 0.0822 - acc: 0.9757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2ba06bd05f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "metadata": {
        "id": "jPqgW345CLdt",
        "colab_type": "code",
        "colab": {},
        "outputId": "71c3bdc5-517a-41e9-8869-9f68b3bf77d1"
      },
      "cell_type": "code",
      "source": [
        "#evaluating the model with the test data now\n",
        "test_loss, test_acc = model.evaluate( test_images, test_labels )\n",
        "print( 'test_acc:', test_acc )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 248us/step\n",
            "test_acc: 0.9803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3bwpB6B_CLdv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We are visualzing the data for the test images. \n",
        "\n",
        "the test_images paramaters are different because we are calling a reshaped test_images, so the parameters have to match accordingly.\n",
        "\n",
        "notice the test_labels is also reshaped. Rather than seeing a value of 0, we get an array of 10 in binary form."
      ]
    },
    {
      "metadata": {
        "id": "yUiGV9R4CLdx",
        "colab_type": "code",
        "colab": {},
        "outputId": "da8785f1-47e3-4b3e-f306-c7ac5f9d5c81"
      },
      "cell_type": "code",
      "source": [
        "plt.matshow( test_images[3, :, :, 0], cmap = 'gray' )\n",
        "plt.show()\n",
        "print( test_labels[3] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fwrAOqP9CLdz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualizing the predictions predictions on the unseen data\n",
        "This is going to show us how the model is predicting the images.\n",
        "\n",
        "The output is an array of values where each value is associated to a digit."
      ]
    },
    {
      "metadata": {
        "id": "88ZhvGptCLd0",
        "colab_type": "code",
        "colab": {},
        "outputId": "53e3938b-f350-4670-9509-7c4d95f2a84b"
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict( test_images )\n",
        "print( predictions[3] )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.0113177e-05 9.9873263e-01 1.6517809e-04 4.9386581e-06 8.3775027e-05\n",
            " 5.8080195e-07 3.7865896e-06 9.6657081e-04 2.1212572e-05 1.3536053e-06]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EO3b5Kn5CLd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After rounding we can see the output of the prediction. Here the model predicted the digit is 0, which is correct."
      ]
    },
    {
      "metadata": {
        "id": "lyTTJas1CLd4",
        "colab_type": "code",
        "colab": {},
        "outputId": "8bfe93ac-773a-43d2-b6ff-593a4f473c92"
      },
      "cell_type": "code",
      "source": [
        "print( predictions[3].round() )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_PbETHIHCLd8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So here we used a convolutional neural network to predict the number for the images of digits. \n",
        "While we only managed ~98-97% we could actually get ~99% if we were able to add more capabilities to the model.\n",
        "\n",
        "There will be a supplimentary file that you can download and try with more time available or better computer."
      ]
    },
    {
      "metadata": {
        "id": "Aiew4WBlCLd9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}